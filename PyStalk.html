<!DOCTYPE html>
<html  >
<head>
  <!-- Site made with Mobirise Website Builder v4.10.4, https://mobirise.com -->
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Mobirise v4.10.4, mobirise.com">
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
  <link rel="shortcut icon" href="assets/images/logo4.png" type="image/x-icon">
  <meta name="description" content="The main page of the PyStalk animal surveillance drone github project">
  
  <title>Main</title>
  <link rel="stylesheet" href="assets/web/assets/mobirise-icons/mobirise-icons.css">
  <link rel="stylesheet" href="assets/tether/tether.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-grid.min.css">
  <link rel="stylesheet" href="assets/bootstrap/css/bootstrap-reboot.min.css">
  <link rel="stylesheet" href="assets/datatables/data-tables.bootstrap4.min.css">
  <link rel="stylesheet" href="assets/theme/css/style.css">
  <link rel="stylesheet" href="assets/mobirise/css/mbr-additional.css" type="text/css">
  
  
  
</head>
<body>
  <section class="header6 cid-rupQMrMsw4 mbr-fullscreen" data-bg-video="https://www.youtube.com/watch?v=ESfWo3vWIFU" id="header6-g">

    

    <div class="mbr-overlay" style="opacity: 0.1; background-color: rgb(35, 35, 35);">
    </div>

    <div class="container">
        <div class="row justify-content-md-center">
            <div class="mbr-white col-md-10">
                <h1 class="mbr-section-title align-center mbr-bold pb-3 mbr-fonts-style display-1">
                    PyStalk
                </h1>
                <p class="mbr-text align-center pb-3 mbr-fonts-style display-5">PyStalk is an autonomous surveillance drone, which uses machine learning and computer vision, that can track animals or people when manually prompted to take off and land.
                </p>
                <div class="mbr-section-btn align-center"><a class="btn btn-md btn-primary display-4" href="https://github.com/Hollyqui/Bebop2-Animal-Tracking/" target="_blank">GitHub Repo</a>
                        <a class="btn btn-md btn-white-outline display-4" href="PyStalk.html#features18-i">DRONE</a></div>
            </div>
        </div>
    </div>

    <div class="mbr-arrow hidden-sm-down" aria-hidden="true">
        <a href="#next">
            <i class="mbri-down mbr-iconfont"></i>
        </a>
    </div>
</section>

<section class="engine"><a href="https://mobirise.info/x">css templates</a></section><section class="mbr-section article content3 cid-rupQMumPLy" id="content3-h">
      
     

    <div class="container">
        <div class="media-container-row">
            <div class="row col-12 col-md-12">
                <div class="col-12 mbr-text mbr-fonts-style col-md-6 display-5">
                     <p><span style="font-size: 1.5rem;">Poaching is recognised as one of the key extinction drivers for endangered species worldwide. With natural biodiversity at risk, new technologies are needed to protect endangered animals.</span><br></p>
                </div>
                <div class="col-12 mbr-text mbr-fonts-style display-7 col-md-6">
                     <p>With 41,415 species acknowledged on the IUCN Red List as endangered, poaching presents a growing threat to Earth’s biodiversity. Poaching is recognised as one of the key extinction drivers for endangered species worldwide. However, traditional methods of combating poachers are no longer enough; new, more efficient methods are required. In this context, the capabilities of artificial intelligence and unmanned aerial vehicles (UAVs) such as drones, to fight poaching efforts will be explored. More specifically, this study used a pre-trained convolutional neural network (CNN) to detect a target autonomously. In this project, the drone camera was used for the detection and tracking of animals and poachers. The objects were detected using a pre-trained TensorFlow object detection model and the drone was accessed remotely using the PyParrot software. All the movement processing and connections between the systems were written by the project team. The results were promising, considering the available time and hardware limitations. The drone motion is not optimal and is relatively slow to react to the movement of the target. However, it can still follow humans at moderate speeds. Thus this project serves as a good proof of concept for a drone that can observe animals. &nbsp;</p>
                </div>
                
            </div>
        </div>
    </div>
</section>

<section class="features18 popup-btn-cards cid-rupQMwkrGx" id="features18-i">

    

    
    <div class="container">
        <h2 class="mbr-section-title pb-3 align-center mbr-fonts-style display-2">
            Drone Project</h2>
        <h3 class="mbr-section-subtitle display-5 align-center mbr-fonts-style mbr-light">Skip to the interesting sections!</h3>
        <div class="media-container-row pt-5 ">
            <div class="card p-3 col-12 col-md-6 col-lg-4">
                <div class="card-wrapper ">
                    <div class="card-img">
                        <div class="mbr-overlay"></div>
                        <div class="mbr-section-btn text-center"><a href="PyStalk.html#content10-n" class="btn btn-primary display-4">Learn More</a></div>
                        <img src="assets/images/img-20190628-145653-41-676x507.jpg" alt="Mobirise" title="">
                    </div>
                    <div class="card-box">
                        <h4 class="card-title mbr-fonts-style display-7">Introduction</h4>
                        <p class="mbr-text mbr-fonts-style align-left display-7">
                            What we did</p>
                    </div>
                </div>
            </div>
            <div class="card p-3 col-12 col-md-6 col-lg-4">
                <div class="card-wrapper">
                    <div class="card-img">
                        <div class="mbr-overlay"></div>
                        <div class="mbr-section-btn text-center"><a href="PyStalk.html#content1-q" class="btn btn-primary display-4">Learn More</a></div>
                        <img src="assets/images/img-20190628-145716-333-676x507.jpg" alt="Mobirise" title="">
                    </div>
                    <div class="card-box">
                        <h4 class="card-title mbr-fonts-style display-7">
                            Materials and Methods</h4>
                        <p class="mbr-text mbr-fonts-style display-7">
                           How we did it</p>
                    </div>
                </div>
            </div>

            <div class="card p-3 col-12 col-md-6 col-lg-4">
                <div class="card-wrapper">
                    <div class="card-img">
                        <div class="mbr-overlay"></div>
                        <div class="mbr-section-btn text-center"><a href="PyStalk.html#content1-s" class="btn btn-primary display-4">Learn More</a></div>
                        <img src="assets/images/img-20190628-145709-743-676x507.jpg" alt="Mobirise" title="">
                    </div>
                    <div class="card-box">
                        <h4 class="card-title mbr-fonts-style display-7">Results</h4>
                        <p class="mbr-text mbr-fonts-style display-7">
                            What we got</p>
                    </div>
                </div>
            </div>

            
        </div>
    </div>
</section>

<section class="features18 popup-btn-cards cid-rupVcuJAPU" id="features18-u">

    

    
    <div class="container">
        
        
        <div class="media-container-row pt-5 ">
            <div class="card p-3 col-12 col-md-6 col-lg-4">
                <div class="card-wrapper ">
                    <div class="card-img">
                        <div class="mbr-overlay"></div>
                        <div class="mbr-section-btn text-center"><a href="PyStalk.html#content1-r" class="btn btn-primary display-4">Learn More</a></div>
                        <img src="assets/images/img-20190628-145716-334-676x507.jpg" alt="Mobirise" title="">
                    </div>
                    <div class="card-box">
                        <h4 class="card-title mbr-fonts-style display-7">Discussion</h4>
                        <p class="mbr-text mbr-fonts-style align-left display-7">
                            Why we got it</p>
                    </div>
                </div>
            </div>
            <div class="card p-3 col-12 col-md-6 col-lg-4">
                <div class="card-wrapper">
                    <div class="card-img">
                        <div class="mbr-overlay"></div>
                        <div class="mbr-section-btn text-center"><a href="PyStalk.html#content1-p" class="btn btn-primary display-4">Learn More</a></div>
                        <img src="assets/images/img-20190628-145658-639-676x507.jpg" alt="Mobirise" title="">
                    </div>
                    <div class="card-box">
                        <h4 class="card-title mbr-fonts-style display-7">
                            Conclusion</h4>
                        <p class="mbr-text mbr-fonts-style display-7">
                           What we think about what we got</p>
                    </div>
                </div>
            </div>

            <div class="card p-3 col-12 col-md-6 col-lg-4">
                <div class="card-wrapper">
                    <div class="card-img">
                        <div class="mbr-overlay"></div>
                        <div class="mbr-section-btn text-center"><a href="PyStalk.html#content1-t" class="btn btn-primary display-4">Learn More</a></div>
                        <img src="assets/images/img-20190628-145653-42-676x507.jpg" alt="Mobirise" title="">
                    </div>
                    <div class="card-box">
                        <h4 class="card-title mbr-fonts-style display-7">
                            Appendix</h4>
                        <p class="mbr-text mbr-fonts-style display-7">Who did what</p>
                    </div>
                </div>
            </div>

            
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rupU9aSh61" id="content1-o">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-5 col-md-8"><p>Introduction and Theory</p></div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruYgYTLcle" id="content2-3">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><p><span style="font-style: normal;">Poaching, which is the illegal hunting, capture or killing of animals, is a global problem: millions of animals from thousands of species are being killed or captured in their native habitats. Elephants, rhinos, and other impressive creatures, as well as smaller and rarer animals like lizards and monkeys, are amongst the many at risk of poaching (Actman, 2019). Poached animals are killed or captured for a variety of reasons, including their commercial values on the black market. Besides being killed for profit, some animals are targeted to prevent them from damaging farms or livestock. Poaching is in some cases the primary reason why a species is at risk of extinction (Meijer et al., 2018).
</span></p><p><span style="font-style: normal;">
</span></p><p><span style="font-style: normal;">The large size of animal habitats makes it difficult to control the area with the traditional surveillance methods, such as security guards. Conservationists are, in an effort to protect animals from poaching, beginning to use unmanned aerial vehicles (UAVs) such as drones to surveil habitats, locate poachers, and track animals.
</span></p><p><span style="font-style: normal;">
</span></p><p><span style="font-style: normal;">According to Olivares-Mendez et al (2015), an ideal anti-poaching drone would include “autonomous taking-off and following of a predefined position list, tracking animals, detecting poachers faces, tracking and following poachers vehicles and return, via an autonomously-landing scenario, on specific stations in order to recharge the batteries and prepare for the next surveillance flight”. 
</span></p><p><span style="font-style: normal;">
</span></p><p><span style="font-style: normal;">Within the constraints of this project, we aim to program an autonomous surveillance drone using machine learning and computer vision that can track animals or people when manually prompted to take off and land, called PyStalk.
</span></p></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruZQPvsHuE" id="content2-n">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><p>Computer Vision and Image Understanding</p><p><span style="font-style: normal;">Computer vision can be seen as an attempt to mimic the human visual system. It can be broken down into four main modules as described in Figure 1 (Ciric et al., 2016).</span></p></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section content6 cid-ruZStmTKAI" id="content6-o">
    
     
    
    <div class="container">
        <div class="media-container-row">
            <div class="col-12 col-md-8">
                <div class="media-container-row">
                    <div class="mbr-figure" style="width: 60%;">
                      <img src="assets/images/computer-vision-block-diagram-474x245.png" alt="Mobirise" title="">  
                    </div>
                    <div class="media-content">
                        <div class="mbr-section-text">
                            <p class="mbr-text mb-0 mbr-fonts-style display-7"><em>Figure 1: Computer vision block-diagram (Ciric et al., 2016)<br></em><br>Image acquisition is defined as the act of capturing and storing an image. After the image acquisition, the image segmentation focuses on dividing the pixels of a picture into regions or categories. The idea is to group the labelled pixels into connected regions according to their category (Glasbey and Horgan, 1995). This preprocessing is meant to facilitate the image understanding module. Reducing the area of interest and pre-labelling facilitates the analyzing process. The image understanding component is the essential and most challenging part of computer vision and consists of analyzing a scene and recognizing all of the constituent objects.&nbsp;</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruZTj3AnFp" id="content2-p">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><span style="font-style: normal;">Two different types of object recognition are distinguishable: generic recognition (or class recognition) and specific object recognition. Generic recognition is defined as generalizing to recognize instances of the same category. Specific object recognition (or instance recognition) differentiates instances within a class of objects (Grauman and Leibe, n.d.). 
<br></span><span style="font-size: 1.09rem;"><span style="font-style: normal;"><br>While computers are able to restore a 3D vision of the real world despite their 2D perception, their image recognition is still considered less efficient than that of a two-year-old child. The key difference between human vision and computer vision lies in the inability of computers to give meaning to an image. Unlike humans who can describe and understand what they see, computers are limited in that regard. They need to be taught what is worth remembering, in order to assign any kind of value to an image. This memory comes in the form of databases with specific labels. These labels act as an explanation of the memories. The issue with recognition can be broken down into several common units. In the case that a computer knows what it is looking for, all it has to do is scan the image for a potential match with the given database. This is known as object detection. The match is generally determined by characteristic feature points and their geometrical alignment stored in the database. Given the myriad of positions an object can adopt, and the possibility of partial occlusion, it becomes clear that generic recognition is already a complex task. For each different potential representation of an object, the computer must be able to categorize them in the same group. Specific object recognition is an even more difficult task: the numerous options of postures and appearances of the same object are unlikely to be stored in a machine’s database (Szeliski, 2011). &nbsp;<br></span></span><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">In an attempt to increase the object recognition efficiency of computers, computer vision has been enhanced by machine learning. Acting as an artificial brain, neural networks come closer to mimic the human visual system.&nbsp;</span></span></blockquote><blockquote>Convolutional Neural Network
<br><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">Recent advancements in computer sciences have seen new and more efficient ways to compute electronic systems. One such advancement is a computer program modeled on the crossed synaptic structure of the brain, Neural Networks (NN). NNs are often used in object classification or, in the context of this project, to identify objects and their position within an image. NNs are usually described in their simplest form as having three principal layers of connected nodes: an input layer, a hidden layer, and an output layer (as illustrated in Figure 2). <br><br>Nodes represent a form of artificial neuron denoted by a number between 0 and 1. Each node of one layer is connected to all the nodes of its adjacent layer by a weight, a random number between 0 and 1. Each node receives multiple inputs, makes a weighted sum over them, and returns the corresponding output (Pokharna, 2019). Each layer plays a crucial role in the network: Firstly, the input layer. It contains all the information that needs to be processed. For example, to feed an image into an input layer each pixel of the image would be assigned to individual nodes in the input layer. These values would correspond to a colour gradient and would be normalised before being fed to the network. <br><br>Secondly, the hidden layer is a layer of nodes that plays a crucial role in adding complexity to the network. It provides more trainable parameters that allow the network to distinguish itself further from the raw inputs (pixels in our example) and come closer to the desired outputs. More hidden layers have to be added when the task increases in difficulty. Multiple hidden layers add even more trainable parameters and therefore allow the network to make better predictions but require greater computational power. <br><br>Lastly, the output layer is a layer that provides the machine analysis of the situation. Each node on the output layer corresponds to a possible outcome. The value assigned to each node corresponds to the confidence level that the machine predicts that outcome. &nbsp;</span></span></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section content6 cid-ruZUnNzwW7" id="content6-q">
    
     
    
    <div class="container">
        <div class="media-container-row">
            <div class="col-12 col-md-8">
                <div class="media-container-row">
                    <div class="mbr-figure" style="width: 60%;">
                      <img src="assets/images/1-rgv6bb3chmvwsa8q6qth6q-624x387.png" alt="Mobirise" title="">  
                    </div>
                    <div class="media-content">
                        <div class="mbr-section-text">
                            <p class="mbr-text mb-0 mbr-fonts-style display-7"><em>Figure 2: Schematic of a Neural Network. <br></em><br>The first input layer is the point of data entries comprised of numpy arrays of numbers normalised between 0 and 1. This data is then fed through the neural network by multiplying each data point (node) by a weight (connection). This weighted sum comprises a single node of the hidden layer. This layer allows for more complexity in the system. The process is repeated until the output layer. This final layer is tested against the expected output and the weights are modified accordingly. This process is called training the network (TechnoReview, 2019).</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruZUrtXKC2" id="content2-r">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><span style="font-style: normal;">Before a NN is able to make accurate predictions it needs to be trained. A large pool of data and their corresponding correct outcomes are fed into the network. The network predicts the outcomes solely based on the input data then compares its prediction with the correct outcome. It evaluates the difference and adjusts its weights accordingly to get more accurate predictions. In doing so, the network changes the values on its weights and nodes and trains itself to identify certain patterns within images. One can appreciate that this entire NN is solely modeled on a weighted sum formula.<br></span><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">Normal NNs work well for basic classifications, however, when more complex tasks are required, they do not provide a sufficiently efficient solution. To combat this, Convolutional Neural Networks (CNN) exist. Unlike NNs, CNNs are applied on volumes: two dimensions of pixel resolution and one for the different colours: red, green, and blue. A one-dimensional filter of resolution smaller than the original image slides along the image. On its path, the dot product between the smaller parts of the image and the filter is computed and recorded. This process is called convolution. Each dot product is a scalar, that forms a “filtered image” as an output. Multiple independent filters are used, each is convolved with the image. In doing so multiple feature maps are created with dimensions smaller than the original image. Like the hidden layers in the NN adding multiple convolutional layers increases the precision and feature extraction of the network.&nbsp;<br></span></span><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">There are a few additional and important features in the context of neural networks. One of these is the use of biases. A bias is an extra value added to a node. Biases can also be trained in combinations with weights. It helps the model adapt more efficiently to its training. Additionally, during training, it is possible that a node is reduced to zero. In this case, no amount of learning (or multiplying by numbers) will change this node. In effect, “killing off” a part of the network. &nbsp;</span></span></blockquote><blockquote>Object Tracking  and Autonomous Drone Navigation
<br><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">Taking one step closer towards human vision, object tracking involves the analysis and interpretation of videos which allow computers to upgrade their representation of the real world. The task of object tracking is the action of finding the position of an object in every frame. The efficiency of object tracking is dependent on the choice of object representation and on how suitable the object tracking method is. As illustrated in Figure 3, object </span></span><span style="font-style: normal;"><span style="font-size: 1.09rem;">tracking</span><span style="font-size: 1.09rem;"> relies on three main techniques: point tracking, kernel tracking and silhouette tracking (Ågren, 2017). This project focussed on kernel tracking, using a quadrilateral shape to determine primitive object region.&nbsp;</span></span></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section content6 cid-ruZWrFf1Cq" id="content6-s">
    
     
    
    <div class="container">
        <div class="media-container-row">
            <div class="col-12 col-md-8">
                <div class="media-container-row">
                    <div class="mbr-figure" style="width: 60%;">
                      <img src="assets/images/annotation-2019-07-01-203223-880x255.png" alt="Mobirise" title="">  
                    </div>
                    <div class="media-content">
                        <div class="mbr-section-text">
                            <p class="mbr-text mb-0 mbr-fonts-style display-7"><em>Figure 3: Object Tracking block-diagram (Ågren, 2017). <br></em><br>The point tracking approach modelizes the detected objects as points and tracks them by evaluating their position and motion. Kernel tracking relies on the computation of the motion of the object represented as primitive object region (frame). The tracking marker is a simple geometric shape enclosing the object. Kernel is referring to the appearance and shape of the object. Silhouette tracking uses kernel tracking to determine the region that matches the model, in the frame.&nbsp;</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruZWsVPlSU" id="content2-t">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><span style="font-style: normal;">When implemented in a mobile device (i.e. a drone), object tracking enables a real-time autonomous navigation capacity. Assuming the consistency of object identification tracking, the main challenge of autonomous navigation is obstacle avoidance, especially in dynamic environments (Ess, et al., 2010). Tracking by detection associates the object detection results with integral trajectories to design a flying path (Lui, et al., 2014). This project will use a 2D image position and will assume that by following the target, obstacle avoidance is guaranteed. 
</span></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rupUlWXXgU" id="content1-q">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-5 col-md-8"><p>Materials and Methods</p></div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruZYijKwIb" id="content2-u">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote>Drone Model &amp; Features
<br><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">The project was conducted with a Bebop2 drone from the Parrot company. Bebop2 has a 25 minutes long flying autonomy and weighs 500 grams. The drone has two flying modes: the videography mode and the sports mode. The camera is adapted for both indoor and outdoor environment ("Parrot Bebop 2", 2019).<br></span></span><span style="font-size: 1.09rem;"><span style="font-style: normal;"><br></span>Tutorials<br></span><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">The PyStalk project can be reproduced by the tutorials stored on the following GitHub repository under the ‘<a href="https://github.com/Hollyqui/PyStalk/tree/master/Tutorial">Tutorial</a>’ folder.</span></span><span style="font-size: 1.09rem;"><span style="font-style: normal;"><br></span></span><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">5 tutorials are available:<br></span></span><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">The “short python intro” can be used to get the basics of Python programming language.<br></span></span><span style="font-style: normal;"><span style="font-size: 1.09rem;"><br>The “intro_to_neural_nets” provides a general introduction to neural networks. The theory is applied to a letter recognition example.<br></span><span style="font-size: 1.09rem;"><br>The “Using Convolutional Neural Networks to classify dogs and cats” provides a general way to use convolutional neural networks to classify images.<br></span><span style="font-size: 1.09rem;"><br>The “TF_model_tutorial” can be used as an instruction guide to install all the packages needed to run the TensorFlow model ssd_inception_v2.<br></span><span style="font-size: 1.09rem;"><br>The “Getting ready with pyparrot_modified” provides an installation guide for all the libraries needed for the drone connectivity and the drone motion.</span></span></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruYi2xdcpL" id="content2-4">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><p>Data Acquisition and Processing
<br><br><span style="font-size: 1.09rem; font-style: normal;">In this project, two data-sets were created for the purpose of training the CNN. The first data-set is comprised of pictures of humans, while the second data-set consists of images of a dog.&nbsp;<br></span><span style="font-size: 1.09rem; font-style: normal;"><br>The data-sets were obtained in two ways: from images and from footage recorded on the drone. The images and videos were taken from multiple angles and heights in order to form a comprehensive training set for the CNN. &nbsp;<br></span><span style="font-size: 1.09rem; font-style: normal;"><br>For the image-based data-set, images were resized to a lower quality to reduce the processing requirements for the neural network. To do this, <a href="https://www.bricelam.net/ImageResizer/" target="_blank">Image Resizer for Windows</a> was used to set the picture quality to 827 x 480.</span></p></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="cid-ruYlsgHrs6" id="video2-b">

    
    
    <figure class="mbr-figure align-center container">
        <div class="video-block" style="width: 60%;">
            <div><iframe class="mbr-embedded-video" src="https://www.youtube.com/embed/BTxoe3UbZgs?rel=0&amp;amp;showinfo=0&amp;autoplay=0&amp;loop=0" width="1280" height="720" frameborder="0" allowfullscreen></iframe></div>
        </div>
    </figure>
</section>

<section class="mbr-section article content1 cid-ruYmfV4y9A" id="content2-c">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><span style="font-style: normal;">For the video-based data-set, the video was split into images using <a href="https://www.videolan.org/vlc/index.html">VLC media player</a>. To do this, the settings of VLC had to be changed. Within Tools&gt;Preferences&gt;Show settings: all&gt; Video&gt;Filters&gt;Scene filter, the image format and size was set to .jpg 827 x 480. The recording ratio, which sets the amount of frames between each extracted image, was set to 24; this means that an image was extracted every 24 frames. 
</span><br></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="cid-ruYmikHh8C" id="video2-d">

    
    
    <figure class="mbr-figure align-center container">
        <div class="video-block" style="width: 60%;">
            <div><iframe class="mbr-embedded-video" src="https://www.youtube.com/embed/BlmlHE2A-QM?rel=0&amp;amp;showinfo=0&amp;autoplay=0&amp;loop=0" width="1280" height="720" frameborder="0" allowfullscreen></iframe></div>
        </div>
    </figure>
</section>

<section class="mbr-section article content1 cid-ruYmjA6Mbj" id="content2-e">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote><p><span style="font-style: normal;">These image-sets then needed to be bounded using <a href="https://github.com/tzutalin/labelImg">labelImg</a>. Bounding is setting a rectangular box around the dog or person in the image; this creates a set of x, y coordinates that inform the CNN where the object of interest is in the picture. The images were randomly allocated to test and train folders for the neural network. &nbsp;
<br></span><span style="font-size: 1.09rem; font-style: normal;"><br>The bounded images then were converted into test and train .csv files - these are spreadsheets listing the coordinates of the bounding boxes in the images. From these .csv files, .tfrecords were made. A <a href="https://medium.com/mostly-ai/tensorflow-records-what-they-are-and-how-to-use-them-c46bc4bbb564">TensorFlow TFRecord</a> is a binary storage format that is optimised for data-set storage, integration and pre-processing. For these format conversions, a <a href="https://github.com/Hollyqui/PyStalk/blob/master/make_TFrecord.py">python script</a> was created to automate this. The .tfrecords are the files used to train the tensorflow object detector</span></p></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-ruZZSjMAA2" id="content2-v">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote>Movement Control 
<br><span style="font-size: 1.09rem;"><br><span style="font-style: normal;">Roll, Pitch, Yaw, and Thrust were the four parameters used in this project to control the movement of the drone. Figure 4 describes these motions in greater detail.&nbsp;</span></span></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section content6 cid-rv0084BjAi" id="content6-w">
    
     
    
    <div class="container">
        <div class="media-container-row">
            <div class="col-12 col-md-8">
                <div class="media-container-row">
                    <div class="mbr-figure" style="width: 60%;">
                      <img src="assets/images/6dof-en-960x635.jpg" alt="Mobirise" title="">  
                    </div>
                    <div class="media-content">
                        <div class="mbr-section-text">
                            <p class="mbr-text mb-0 mbr-fonts-style display-7"><br><em>Figure 4: Schematic representation of the drone motion commands (Emissary Drones, 2019) <br><br></em>The change in yaw determines the rotation of the drone and allows it to track an object that is moving left or right. Roll allows the drone to move left or right. This allows the same tracking capability as yaw by moving the drone rather than rotating it. Pitch allows the drone to move forwards or backwards and therefore regulate its distance to the target. The vertical motion is controlled by the thrust of the propellers. This project implemented a fixed vertical motion. Manually the drone height could be adjusted.</p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rv009dNbKY" id="content2-x">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote>Box Smoothing
<br><span style="font-style: normal; font-size: 1.09rem;"><br>Given the unreliability of the object recognition and the potential occlusion of objects (e.g. vegetation), the tracking is still experimental. The possible issues laid in a lack of object detection in some frames of the video. In response to this, a “smoothened box” was implemented. This box solves the issues by taking a weighted average of the location of the detected boxes of the last 30 frames. In doing so, the detection is made smoother and a possible sudden jump, causing an acceleration of the drone, is everted. The number of frames considered for the weighted average is modifiable. This particular implementation allowed the drone to compensate for the lack of boxes over a period of about one second (frames come in at a rate of 25/second).&nbsp;<br></span><span style="font-size: 1.09rem; font-style: normal;"><br>Currently, the frames are weighted as n^5, where n is the position of the frame. This means that the last frames weigh much more than the first frames. This is a good solution however, it is likely that an exponential weighting (e^n) would have been more efficient in updating smoothened boxes faster. This was not experimentally confirmed however, it remains highly probable. Due to the delay with the WIFI transmissions, the polynomial weighting is deemed to be less efficient as it causes an additional delay when objects are moving quickly. &nbsp;<br></span><span style="font-size: 1.09rem; font-style: normal;"><br>The second computation done by the box smoothing function selects one of the potentially multiple recognized objects for the tracking. This is done by choosing the box with the highest recognition certainty in the first frame. In every subsequent frame, the computation is implemented to check which of the boxes around the recognized objects is closest to the smoothened box. This way, a sort of ‘understanding’ of object permanence can be achieved. This is important to avoid the drone flying towards other animals that are not supposed to be tracked. One current issue is that the box switches away from one target very quickly when it is not recognized anymore. Instead, it finds another target since it assumes that the object moves very quickly.&nbsp;</span></blockquote><blockquote>
<span style="font-size: 1.09rem;">Rotational vs Fixed Tracking&nbsp;<br></span><span style="font-style: normal; font-size: 1.09rem;"><br>The drone can operate on two different tracking modes; rotational and fixed. If rotational tracking is activated yaw and pitch are used and the roll stays constant. This tracking is much more dynamic and far more adept at tracking in open areas. The downside is that as GPS stabilization is not activated the drone tends to drift to the side. If fixed tracking is activated, the rotation is set on a given angle and the drone moves by only using roll and pitch. The issue of drifting is no longer present as the left/right motion is relative to the object. In other words, if the object doesn’t move, the drone won’t move either. That said, the drone has a tendency to lose the object that it is tracking despite the countermeasures in place.&nbsp;</span><span style="font-style: normal; font-size: 1.09rem;"></span></blockquote><blockquote>Pitch<span style="font-style: normal;">
<br></span><span style="font-style: normal; font-size: 1.09rem;"><br>The drone motion is implemented so that the drone will constantly regulate its position according to the distance to the object. This distance is specified by a “desired box size”. The latter represents the area on the screen that the user would like the tracked object to cover.  This means that the drone will try to move further away from the object if it notices that more than the desired percentage of the screen is covered and vice versa. This desired distance is defined by two boxes (which are displayed in the GUI in light blue). One box is the maximum amount of the screen that should be covered and the other box represents the minimum amount of screen that should be covered. Consequently, the drone will adjust its distance so that the size of the object remains between those two boxes.&nbsp;<br></span><span style="font-style: normal; font-size: 1.09rem;"><br>The computation for this is not trivial and is currently expressed as the set_distance() function in the Movement_processing code file: &nbsp;</span></blockquote>
            </div>
        </div>
    </div>
</section>

<section class="features4 cid-ruYM5VG9zR" id="features4-f">
    
         

    
    <div class="container">
        <div class="media-container-row">
            <div class="card p-3 col-12 col-md-6">
                <div class="card-wrapper media-container-row media-container-row">
                    <div class="card-box">
                        
                        <p class="mbr-text mbr-fonts-style display-7">def set_distance(self):
<br>
<br>   if (self.box_size &gt; 0):
<br>       if self.box_size &gt; self.max_distance:
<br>           self.pitch = (self.box_size - self.max_distance) * -3
<br>       elif self.box_size &lt; self.min_distance:
<br>           self.pitch = (self.box_size - self.min_distance) * -3
<br>       else:
<br>           self.pitch = 0&nbsp;<br></p>
                    </div>
                </div>
            </div>

            

            

            
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rv02U5e9XL" id="content2-y">

     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 col-md-8 mbr-fonts-style display-7">
                <blockquote> <strong>Make your own website in a few clicks!</strong> Mobirise helps you cut down development time by providing you with a flexible website editor with a drag and drop interface. Mobirise Website Builder creates responsive, retina and <strong>mobile friendly websites</strong> in a few clicks. Mobirise is one of the easiest website development tools <a href="https://mobirise.co/">available</a> today. It also gives you the freedom to develop as many websites as you like given the fact that it is a desktop app.</blockquote>
            </div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rupUnWEpXv" id="content1-s">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-5 col-md-8"><p>Results&nbsp;&nbsp;</p></div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rupUmKLRoL" id="content1-r">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-5 col-md-8"><p>Discussion</p></div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rupUaT6NRa" id="content1-p">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-5 col-md-8"><p>Conclusion</p></div>
        </div>
    </div>
</section>

<section class="mbr-section article content1 cid-rupUzFUu6U" id="content1-t">
    
     

    <div class="container">
        <div class="media-container-row">
            <div class="mbr-text col-12 mbr-fonts-style display-5 col-md-8"><p>Appendix</p></div>
        </div>
    </div>
</section>

<section class="section-table cid-ruZ6lCZDix" id="table1-m">

  
  
  <div class="container container-table">
      
      
      <div class="table-wrapper">
        <div class="container">
          
        </div>

        <div class="container scroll">
          <table class="table" cellspacing="0">
            <thead>
              <tr class="table-heads ">
                  
                  
                  
                  
              <th class="head-item mbr-fonts-style display-7">Practical Work</th><th class="head-item mbr-fonts-style display-7"></th></tr>
            </thead>

            <tbody>
              
              
              
              
            <tr> 
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Data acquistion and processing</td><td class="body-item mbr-fonts-style display-7">Fenora and Felix</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Installation (libraries, dependencies…)</td><td class="body-item mbr-fonts-style display-7">Felix, Szymon, Timour and Siane</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Motion project
<br></td><td class="body-item mbr-fonts-style display-7">Felix, Szymon and Siane</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Neural network project
<br></td><td class="body-item mbr-fonts-style display-7">Timour</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Project combination
<br></td><td class="body-item mbr-fonts-style display-7">Felix</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Testing of general project (tracking with drone)
<br></td><td class="body-item mbr-fonts-style display-7">Everyone</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7"><strong>Reporting</strong></td><td class="body-item mbr-fonts-style display-7"></td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Abstract</td><td class="body-item mbr-fonts-style display-7">Szymon and Fenora</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Introduction</td><td class="body-item mbr-fonts-style display-7">Timour, Fenora and Siane</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Material and Methods</td><td class="body-item mbr-fonts-style display-7">Fenora, Szymon and Siane</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Tutorials (Jupyter notebooks)</td><td class="body-item mbr-fonts-style display-7">Felix, Szymon, Siane and Timour</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Results</td><td class="body-item mbr-fonts-style display-7">Fenora and Felix</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Discussion</td><td class="body-item mbr-fonts-style display-7">Szymon and Felix</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Conclusion and Future Ideas</td><td class="body-item mbr-fonts-style display-7">Siane, Szymon and Felix</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Formatting</td><td class="body-item mbr-fonts-style display-7">Everyone</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Website</td><td class="body-item mbr-fonts-style display-7">Fenora</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">GitHub Repository Editing</td><td class="body-item mbr-fonts-style display-7">Felix</td></tr><tr>
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Appendix</td><td class="body-item mbr-fonts-style display-7">Siane</td></tr></tbody>
          </table>
        </div>
        <div class="container table-info-container">
          
        </div>
      </div>
    </div>
</section>

<section class="section-table cid-ruZ1B5ky7L" id="table1-l">

  
  
  <div class="container container-table">
      
      
      <div class="table-wrapper">
        <div class="container">
          
        </div>

        <div class="container scroll">
          <table class="table" cellspacing="0">
            <thead>
              <tr class="table-heads ">
                  
                  
                  
                  
              <th class="head-item mbr-fonts-style display-7">
                      NAME</th><th class="head-item mbr-fonts-style display-7">
                      ID</th></tr>
            </thead>

            <tbody>
              
              
              
              
            <tr> 
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Felix Quinque</td><td class="body-item mbr-fonts-style display-7">i6166368</td></tr><tr> 
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Szymon Fonau</td><td class="body-item mbr-fonts-style display-7">i6161129</td></tr><tr> 
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Fenora Mc Kiernan</td><td class="body-item mbr-fonts-style display-7">i6156417</td></tr><tr> 
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Timour Javar Magnier</td><td class="body-item mbr-fonts-style display-7">i6175968</td></tr><tr> 
                
                
                
                
              <td class="body-item mbr-fonts-style display-7">Siane Lemoine</td><td class="body-item mbr-fonts-style display-7">i6159295</td></tr></tbody>
          </table>
        </div>
        <div class="container table-info-container">
          
        </div>
      </div>
    </div>
</section>


  <script src="assets/web/assets/jquery/jquery.min.js"></script>
  <script src="assets/popper/popper.min.js"></script>
  <script src="assets/tether/tether.min.js"></script>
  <script src="assets/bootstrap/js/bootstrap.min.js"></script>
  <script src="assets/ytplayer/jquery.mb.ytplayer.min.js"></script>
  <script src="assets/vimeoplayer/jquery.mb.vimeo_player.js"></script>
  <script src="assets/mbr-popup-btns/mbr-popup-btns.js"></script>
  <script src="assets/datatables/jquery.data-tables.min.js"></script>
  <script src="assets/datatables/data-tables.bootstrap4.min.js"></script>
  <script src="assets/smoothscroll/smooth-scroll.js"></script>
  <script src="assets/theme/js/script.js"></script>
  
  
</body>
</html>